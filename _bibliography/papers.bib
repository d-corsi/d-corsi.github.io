---
2023
---

@inproceedings{corsi2023constrained,
  title={Constrained Reinforcement Learning and Formal Verification for Safe Colonoscopy Navigation},
  author={Corsi*, Davide and Marzari*, Luca and Pore*, Ameya and Farinelli, Alessandro and Casals, Alicia and  Fiorini, Paolo and Dall'Alba, Diego},
  booktitle={IEEE International Conference on Intelligent Robots and Systems},
  year={2023},
  abbr={IROS},
  selected={true},
  pdf={https://arxiv.org/pdf/2303.03207.pdf},
  html={https://arxiv.org/abs/2303.03207},
  abstract={The field of robotic Flexible Endoscopes (FEs) has progressed significantly, offering a promising solution to reduce patient discomfort. However, the limited autonomy of most robotic FEs results in non-intuitive and challenging manoeuvres, constraining their application in clinical settings. While previous studies have employed lumen tracking for autonomous navigation, they fail to adapt to the presence of obstructions and sharp turns when the endoscope faces the colon wall. In this work, we propose a Deep Reinforcement Learning (DRL)-based navigation strategy that eliminates the need for lumen tracking. However, the use of DRL methods poses safety risks as they do not account for potential hazards associated with the actions taken. To ensure safety, we exploit a Constrained Reinforcement Learning (CRL) method to restrict the policy in a predefined safety regime. Moreover, we present a model selection strategy that utilises Formal Verification (FV) to choose a policy that is entirely safe before deployment. We validate our approach in a virtual colonoscopy environment and report that out of the 300 trained policies, we could identify three policies that are entirely safe. Our work demonstrates that CRL, combined with model selection through FV, can improve the robustness and safety of robotic behaviour in surgical applications.}
}

@inproceedings{marzari2021DNNverification,
  title={The \#DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks},
  author={Marzari*, Luca and Corsi*, Davide and Cicalese, Ferdinando and Farinelli, Alessandro},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2023},
  abbr={IJCAI},
  selected={true},
  pdf={https://arxiv.org/pdf/2301.07068.pdf},
  html={https://arxiv.org/abs/2301.07068},
  abstract={Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-critical benchmarks that demonstrate the effectiveness of our approximate method and evaluate the tightness of the bound.}
}

@inproceedings{amir2023verifying,
  title={Verifying Learning-Based Robotic Navigation Systems},
  author={Amir*, Guy and Corsi*, Davide and Yerushalmi, Raz and Marzari, Luca and Farinelli, Alessandro and Harel, David and Katz, Guy},
  booktitle={Tools and Algorithms for the Construction and Analysis of Systems},
  year={2023},
  abbr={TACAS},
  selected={true},
  pdf={https://arxiv.org/pdf/2205.13536.pdf},
  html={https://link.springer.com/chapter/10.1007/978-3-031-30823-9_31},
  abstract={Deep reinforcement learning (DRL) has become a dominant deep-learning paradigm for tasks where complex policies are learned within reactive systems. Unfortunately, these policies are known to be susceptible to bugs. Despite significant progress in DNN verification, there has been little work demonstrating the use of modern verification tools on real-world, DRL-controlled systems. In this case study, we attempt to begin bridging this gap, and focus on the important task of mapless robotic navigation â€” a classic robotics problem, in which a robot, usually controlled by a DRL agent, needs to efficiently and safely navigate through an unknown arena towards a target. We demonstrate how modern verification engines can be used for effective model selection, i.e., selecting the best available policy for the robot in question from a pool of candidate policies. Specifically, we use verification to detect and rule out policies that may demonstrate suboptimal behavior, such as collisions and infinite loops. We also apply verification to identify models with overly conservative behavior, thus allowing users to choose superior policies, which might be better at finding shorter paths to a target. To validate our work, we conducted extensive experiments on an actual robot, and confirmed that the suboptimal policies detected by our method were indeed flawed. We also demonstrate the superiority of our verification-driven approach over state-of-the-art, gradient attacks. Our work is the first to establish the usefulness of DNN verification in identifying and filtering out suboptimal DRL policies in real-world robots, and we believe that the methods presented here are applicable to a wide range of systems that incorporate deep-learning-based agents.}
}

@inproceedings{corsi2023constrained,
  title={Constrained Reinforcement Learning for Robotics via Scenario-Based Programming},
  author={Corsi*, Davide and Yerushalmi*, Raz and Amir, Guy and Farinelli, Alessandro and Harel, David and Katz, Guy},
  booktitle={arXiv preprint arXiv:2206.09603},
  year={2023},
  abbr={arXiv},
  selected={false},
  pdf={https://arxiv.org/pdf/2206.09603.pdf},
  html={https://arxiv.org/abs/2206.09603},
  abstract={Deep reinforcement learning (DRL) has achieved groundbreaking successes in a wide variety of robotic applications. A natural consequence is the adoption of this paradigm for safety-critical tasks, where human safety and expensive hardware can be involved. In this context, it is crucial to optimize the performance of DRL-based agents while providing guarantees about their behavior. This paper presents a novel technique for incorporating domain-expert knowledge into a constrained DRL training loop. Our technique exploits the scenario-based programming paradigm, which is designed to allow specifying such knowledge in a simple and intuitive way. We validated our method on the popular robotic mapless navigation problem, in simulation, and on the actual platform. Our experiments demonstrate that using our approach to leverage expert knowledge dramatically improves the safety and the performance of the agent.}
}

@inproceedings{Corsi2023thesis,
  title={Safe Deep Reinforcement Learning: Enhancing the Reliability of Intelligent Systems},
  author={Corsi, Davide},
  booktitle={University of Verona},
  year={2023},
  abbr={PhD Thesis},
  selected={false},
  pdf={https://d-corsi.github.io/assets/pdf/Corsi2023thesis.pdf},
  abstract={
  	In the last few years, the impressive success of deep reinforcement learning (DRL) agents in a wide variety of applications has led to the adoption of these systems in safety-critical contexts (e.g., autonomous driving, robotics, and medical applications), where expensive hardware and human safety can be involved. In such contexts, an intelligent learning agent must adhere to certain requirements that go beyond the simple accomplishment of the task and typically include constraints on the agent's behavior. Against this background, this thesis proposes a set of training and validation methodologies that constitute a unified pipeline to generate safe and reliable DRL agents. In the first part of this dissertation, we focus on the problem of constrained DRL, leaving the challenging problem of the formal verification of deep neural networks for the second part of this work.

	As humans, in our growing process, the help of a mentor is crucial to learn effective strategies to solve a problem while a learning process driven only by a trial-and-error approach usually leads to unsafe and inefficient solutions. Similarly, a pure end-to-end deep reinforcement learning approach often results in suboptimal policies, which typically translates into unpredictable, and thus unreliable, behaviors.
	Following this intuition, we propose to impose a set of constraints into the DRL loop to guide the training process. These requirements, which typically encode domain expert knowledge, can be seen as suggestions that the agent should follow but is allowed to sometimes ignore if useful to maximize the reward signal.
	A foundational requirement for our work is finding a proper strategy to define and formally encode these constraints (which we refer to as rules). In this thesis, we propose to exploit a formal language inherited from the software engineering community: scenario-based programming (SBP). For the actual training, we rely on the constrained reinforcement learning paradigm, proposing an extended version of the Lagrangian PPO algorithm. 

	Recalling the parallelism with human beings, before being authorized to perform safety-critical operations, we must obtain a certification (e.g., a license to drive a car or a degree to perform medical operations).
	In the second part of this dissertation, we apply this concept in a deep reinforcement learning context, where the intelligent agents are controlled by artificial neural networks. In particular, we propose to perform a model selection phase after the training to find models that formally respect some given safety requirements before the deployment. However, DNNs have long been considered unpredictable black boxes and thus unsuitable for safety-critical contexts. Against this background, we build upon the emerging field of formal verification for neural networks to extend state-of-the-art approaches to robotic decision-making contexts. We propose "ProVe", a verification tool for decision-making DNNs that quantifies the probability of violating the specified requirements. In the last chapter of this thesis, we provide a complete case study on a popular robotic problem: "mapless navigation". Here, we show a concrete example of the application of our pipeline, starting from the definition of the requirements to the training and the final formal verification phase, to finally obtain a provably safe and effective agent.}
}

---
2022
---

@inproceedings{marchesini2022exploring,
  title={Exploring Safer Behaviors for Dep Reinforcement Learning},
  author={Marchesini*, Enrico and Corsi*, Davide and Farinelli, Alessandro},
  booktitle={The 36th AAAI Conference on Artificial Intelligence},
  year={2022},
  abbr={AAAI},
  selected={true},
  pdf={https://ojs.aaai.org/index.php/AAAI/article/view/20737/},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/20737/},
  abstract={We consider Reinforcement Learning (RL) problems where an agent attempts to maximize a reward signal while minimizing a cost function that models unsafe behaviors. Such formalization is addressed in the literature using constrained optimization on the cost, limiting the exploration and leading to a significant trade-off between cost and reward. In contrast, we propose a Safety-Oriented Search that complements Deep RL algorithms to bias the policy toward safety within an evolutionary cost optimization. We leverage evolutionary exploration benefits to design a novel concept of safe mutations that use visited unsafe states to explore safer actions. We further characterize the behaviors of the policies over desired specifics with a sample-based bound estimation, which makes prior verification analysis tractable in the training loop. Hence, driving the learning process towards safer regions of the policy space. Empirical evidence on the Safety Gym benchmark shows that we successfully avoid drawbacks on the return while improving the safety of the policy.}
}

@inproceedings{marzari2022curriculum,
  title={Curriculum Learning for Safe Mapless Navigation},
  author={Marzari, Luca and Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro},
  booktitle={ACM/SIGAPP Symposium on Applied Computing},
  year={2022},
  abbr={ACM/SAC},
  selected={false},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3477314.3507182},
  html={https://dl.acm.org/doi/abs/10.1145/3477314.3507182/},
  abstract={This work investigates the effects of Curriculum Learning (CL)-based approaches on the agent's performance. In particular, we focus on the safety aspect of robotic mapless navigation, comparing over a standard end-to-end (E2E) training strategy. To this end, we present a CL approach that leverages Transfer of Learning (ToL) and fine-tuning in a Unity-based simulation with the Robotnik Kairos as a robotic agent. For a fair comparison, our evaluation considers an equal computational demand for every learning approach (i.e., the same number of interactions and difficulty of the environments) and confirms that our CL-based method that uses ToL outperforms the E2E methodology. In particular, we improve the average success rate and the safety of the trained policy, resulting in 10\% fewer collisions in unseen testing scenarios. To further confirm these results, we employ a formal verification tool to quantify the number of correct behaviors of Reinforcement Learning policies over desired specifications.}
}

---
2021
---

}

@inproceedings{corsi2021benchmarking,
  title={Benchmarking Safe Deep Reinforcement Learning in Aquatic Navigation},
  author={Corsi*, Davide and Marchesini*, Enrico and Farinelli, Alessandro},
  booktitle={IEEE International Conference on Intelligent Robots and Systems},
  year={2021},
  abbr={IROS},
  selected={false},
  html={https://ieeexplore.ieee.org/abstract/document/9635925/},
  abstract={We propose a novel benchmark environment for Safe Reinforcement Learning focusing on aquatic navigation. Aquatic navigation is an extremely challenging task due to the non-stationary environment and the uncertainties of the robotic platform, hence it is crucial to consider the safety aspect of the problem, by analyzing the behavior of the trained network to avoid dangerous situations (e.g., collisions). To this end, we consider a value-based and policy-gradient Deep Reinforcement Learning (DRL) and we propose a crossover-based strategy that combines gradient-based and gradient-free DRL to improve sample-efficiency. Moreover, we propose a verification strategy based on interval analysis that checks the behavior of the trained models over a set of desired properties. Our results show that the crossover-based training outperforms prior DRL approaches, while our verification allows us to quantify the number of configurations that violate the behaviors that are described by the properties. Crucially, this will serve as a benchmark for future research in this domain of applications.}
}

@inproceedings{pore2021safe,
  title={Safe Reinforcement Learning Using Formal Verification for Tissue Retraction in Autonomous Robotic-Assisted Surgery},
  author={Pore*, Ameya and Corsi*, Davide and Marchesini*, Enrico and Dall'Alba, Diego and Casals, Alicia and Farinelli, Alessandro  and Fiorini, Paolo},
  booktitle={IEEE International Conference on Intelligent Robots and Systems},
  year={2021},
  abbr={IROS},
  selected={true},
  html={https://ieeexplore.ieee.org/abstract/document/9636175/},
  abstract={Deep Reinforcement Learning (DRL) is a viable solution for automating repetitive surgical subtasks due to its ability to learn complex behaviours in a dynamic environment. This task automation could lead to reduced surgeonâ€™s cognitive workload, increased precision in critical aspects of the surgery, and fewer patient-related complications. However, current DRL methods do not guarantee any safety criteria as they maximise cumulative rewards without considering the risks associated with the actions performed. Due to this limitation, the application of DRL in the safety-critical paradigm of robot-assisted Minimally Invasive Surgery (MIS) has been constrained. In this work, we introduce a Safe-DRL framework that incorporates safety constraints for the automation of surgical subtasks via DRL training. We validate our approach in a virtual scene that replicates a tissue retraction task commonly occurring in multiple phases of an MIS. Furthermore, to evaluate the safe behaviour of the robotic arms, we formulate a formal verification tool for DRL methods that provides the probability of unsafe configurations. Our results indicate that a formal analysis guarantees safety with high confidence such that the robotic instruments operate within the safe workspace and avoid hazardous interaction with other anatomical structures.}
}

@inproceedings{corsi2021formal,
  title={Formal Verification of Neural Networks for Safety-Critical Tasks in Deep Reinforcement Learning},
  author={Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro},
  booktitle={The 37th Conference on Uncertainty in Artificial Intelligence},
  year={2021},
  abbr={UAI},
  selected={true},
  pdf={https://proceedings.mlr.press/v161/corsi21a/corsi21a.pdf},
  html={https://proceedings.mlr.press/v161/corsi21a.html},
  abstract={In the last years, neural networks achieved groundbreaking successes in a wide variety of applications. However, for safety critical tasks, such as robotics and healthcare, it is necessary to provide some specific guarantees before the deployment in a real world context. Even in these scenarios, where high cost equipment and human safety are involved, the evaluation of the models is usually performed with the standard metrics (i.e., cumulative reward or success rate). In this paper, we introduce a novel metric for the evaluation of models in safety critical tasks, the violation rate. We build our work upon the concept of formal verification for neural networks, providing a new formulation for the safety properties that aims to ensure that the agent always makes rational decisions. To perform this evaluation, we present ProVe (Property Verifier), a novel approach based on the interval algebra, designed for the analysis of our novel behavioral properties. We apply our method to different domains (i.e., mapless navigation for mobile robots, trajectory generation for manipulators, and the standard ACAS benchmark). Results show that the violation rate computed by ProVe provides a good evaluation for the safety of trained models.} 
}

---
2020
---

@inproceedings{marchesini2020genetic,
  title={Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning},
  author={Marchesini, Enrico and Corsi, Davide and Farinelli, Alessandro},
  booktitle={The 9th International Conference on Learning Representations},
  year={2020},
  abbr={ICLR},
  selected={false},
  pdf={https://openreview.net/pdf?id=TGFO0DbD_pk},
  html={https://openreview.net/forum?id=TGFO0DbD_pk} 
}

@inproceedings{corsi2020formal,
  title={Formal verification for Safe Deep Reinforcement Learning in Trajectory Generation},
  author={Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro and Fiorini, Paolo, },
  booktitle={IEEE International Conference on Robotic Computing},
  year={2020},
  abbr={IRC},
  selected={false},
  html={https://ieeexplore.ieee.org/abstract/document/9287929/}
}

@inproceedings{marchesini2020double,
  title={Double Deep Q-Network for Trajectory Generation of a Commercial 7DOF Redundant Manipulator},
  author={Marchesini, Enrico and Corsi, Davide and Benfatti, Andrea and Farinelli, Alessandro and Fiorini, Paolo},
  booktitle={IEEE International Conference on Robotic Computing},
  year={2020},
  abbr={IRC},
  selected={false},
  html={https://ieeexplore.ieee.org/abstract/document/8675598}
}
